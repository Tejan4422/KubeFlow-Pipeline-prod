{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimised RAG Pipeline\n",
        "\n",
        "## 1. Pipeline Architecture\n",
        "### 2. Extract Queries from XLSX\n",
        "\n",
        "\n",
        "1.   Read all sheets in the uploaded XLSX file.\n",
        "2.   Combine them into a single DataFrame while preserving sheet names as headers.\n",
        "\n",
        "3. Save the transformed DataFrame as a CSV file in a \"processed_input\" folder in the same bucket.\n",
        "\n",
        "\n",
        "4. Process Queries : Read the transformed CSV file.\n",
        "5. Process each query by:\n",
        "  \n",
        "  *   Checking Firestore for a cached response.\n",
        "\n",
        "  *   Calling the Discovery Engine API if not found.\n",
        "\n",
        "  *   Storing response embeddings in Firestore.\n",
        "\n",
        "  *   Logging queries & responses into BigQuery.\n",
        "  *  Add the responses as a new column.\n",
        "    \n",
        "  * Save the updated DataFrame in a \"processed_output\" folder in the same bucket.\n",
        "  * Save Final Results\n",
        "\n",
        "### 3. Upload the processed CSV file to the Cloud Storage bucket.\n"
      ],
      "metadata": {
        "id": "RzMtRoS2vPbo"
      },
      "id": "RzMtRoS2vPbo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "rmIsVuXxfc6vA9Dw6m1ghBSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a260f7d4-8861-40a3-9465-027dde1c99f9"
      },
      "source": [
        "!pip install google-cloud-aiplatform kfp google-cloud-storage google-cloud-firestore google-cloud-bigquery sentence-transformers requests pandas openpyxl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.74.0)\n",
            "Collecting kfp\n",
            "  Downloading kfp-2.12.1.tar.gz (345 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/345.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m256.0/345.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-cloud-firestore in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (4.25.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.14.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.10.4)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: click<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (8.1.8)\n",
            "Collecting kfp-pipeline-spec==0.6.0 (from kfp)\n",
            "  Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl.metadata (293 bytes)\n",
            "Collecting kfp-server-api<2.5.0,>=2.1.0 (from kfp)\n",
            "  Downloading kfp_server_api-2.4.0.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kubernetes<31,>=8.0.0 (from kfp)\n",
            "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: PyYAML<7,>=5.3 in /usr/local/lib/python3.10/dist-packages (from kfp) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from kfp) (1.0.0)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.10/dist-packages (from kfp) (0.9.0)\n",
            "Collecting urllib3<2.0.0 (from kfp)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kfp-server-api<2.5.0,>=2.1.0->kfp) (1.17.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<31,>=8.0.0->kfp) (3.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading kfp_pipeline_spec-0.6.0-py3-none-any.whl (9.1 kB)\n",
            "Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kfp, kfp-server-api\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-2.12.1-py3-none-any.whl size=366348 sha256=79b713dca26332d264fe966e569d87d8e589c5d0cc2483b8cd0b2fd12d418566\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a4/89/6f1fa2a3dae3976bc14d70e368e4064be8d4b0628af0ef7b85\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-2.4.0-py3-none-any.whl size=116526 sha256=a8ee38f1919017efe573c788b4dddfd39d83b5357246f91ea536d37b25573b58\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/0f/30/65abccda2186c59a28fd37c13675dca68d4e3b9e15f731107d\n",
            "Successfully built kfp kfp-server-api\n",
            "Installing collected packages: urllib3, kfp-pipeline-spec, kfp-server-api, kubernetes, kfp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "Successfully installed kfp-2.12.1 kfp-pipeline-spec-0.6.0 kfp-server-api-2.4.0 kubernetes-30.1.0 urllib3-1.26.20\n"
          ]
        }
      ],
      "id": "rmIsVuXxfc6vA9Dw6m1ghBSa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "uxFEUUGowbDi"
      },
      "id": "uxFEUUGowbDi"
    },
    {
      "cell_type": "code",
      "source": [
        "import kfp\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2.dsl import (Artifact, Dataset, Input, Output, component, Model)\n",
        "from google.cloud import storage, firestore, bigquery\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize Sentence-BERT Model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "n5DO-HSzvBzN"
      },
      "id": "n5DO-HSzvBzN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Queries from xlsx Component"
      ],
      "metadata": {
        "id": "kZPqgoqRwfaq"
      },
      "id": "kZPqgoqRwfaq"
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(base_image=\"python:3.9\")\n",
        "def extract_queries_from_xlsx(\n",
        "    source_bucket: str, file_name: str, results_bucket: str\n",
        ") -> str:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-storage\", \"pandas\", \"openpyxl\"], check=True)\n",
        "\n",
        "    from google.cloud import storage\n",
        "    import pandas as pd\n",
        "    import os\n",
        "\n",
        "    # Initialize Cloud Storage Client\n",
        "    storage_client = storage.Client()\n",
        "    source_bucket_ref = storage_client.bucket(source_bucket)\n",
        "    results_bucket_ref = storage_client.bucket(results_bucket)\n",
        "    blob = source_bucket_ref.blob(file_name)\n",
        "\n",
        "    # Define Paths\n",
        "    local_xlsx_path = f\"/tmp/{file_name}\"\n",
        "    processed_file_name = file_name.replace('.xlsx', '.csv')\n",
        "    processed_gcs_path = f\"extracted_queries/{processed_file_name}\"  # ✅ Save in `extracted_queries/`\n",
        "    local_output_path = f\"/tmp/{processed_file_name}\"\n",
        "\n",
        "    # Download XLSX File\n",
        "    blob.download_to_filename(local_xlsx_path)\n",
        "    print(f\"Downloaded {file_name} to {local_xlsx_path}\")\n",
        "\n",
        "    # Read all sheets in the XLSX file\n",
        "    xls = pd.ExcelFile(local_xlsx_path)\n",
        "    all_data = []\n",
        "\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
        "        if not df.empty:\n",
        "            df.insert(0, \"Sheet_Name\", sheet_name)  # ✅ Preserve Sheet Name\n",
        "            all_data.append(df)\n",
        "\n",
        "    if not all_data:\n",
        "        raise ValueError(\"No data found in any sheets.\")\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    # Save combined DataFrame as CSV\n",
        "    combined_df.to_csv(local_output_path, index=False)\n",
        "\n",
        "    # Upload to the new results bucket inside `extracted_queries/`\n",
        "    blob = results_bucket_ref.blob(processed_gcs_path)\n",
        "    blob.upload_from_filename(local_output_path)\n",
        "\n",
        "    print(f\"Processed queries saved in GCS: {processed_gcs_path}\")\n",
        "\n",
        "    return processed_gcs_path  # ✅ Output GCS path for next component\n",
        "\n"
      ],
      "metadata": {
        "id": "qg3ZPRJJvCTh"
      },
      "id": "qg3ZPRJJvCTh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use BERT to filter similar queries from the Corpus"
      ],
      "metadata": {
        "id": "9OC3Vm9RwpzT"
      },
      "id": "9OC3Vm9RwpzT"
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(base_image=\"python:3.9\")\n",
        "def filter_similar_queries(\n",
        "    results_bucket: str, processed_file_path: str, firestore_project_id: str\n",
        ") -> list:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                   \"google-cloud-firestore\", \"google-cloud-storage\",\n",
        "                   \"pandas\", \"sentence-transformers\", \"scikit-learn\"], check=True)\n",
        "\n",
        "    import pandas as pd\n",
        "    from google.cloud import firestore, storage\n",
        "    import hashlib\n",
        "    import json\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Initialize Clients\n",
        "    storage_client = storage.Client()\n",
        "    db = firestore.Client(project=firestore_project_id)\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Define Paths\n",
        "    local_csv_path = f\"/tmp/{processed_file_path.split('/')[-1]}\"\n",
        "    results_bucket_ref = storage_client.bucket(results_bucket)\n",
        "    blob = results_bucket_ref.blob(processed_file_path)\n",
        "\n",
        "    # Download the processed CSV\n",
        "    blob.download_to_filename(local_csv_path)\n",
        "    df = pd.read_csv(local_csv_path)\n",
        "\n",
        "    # Detect Requirement Column\n",
        "    req_columns = [col for col in df.columns if 'requirements' in col.lower()]\n",
        "    if not req_columns:\n",
        "        raise ValueError(\"No requirement column found in the processed input.\")\n",
        "\n",
        "    requirement_col = req_columns[0]\n",
        "\n",
        "    # Prepare data structures for results\n",
        "    cached_queries = []\n",
        "    new_queries = []\n",
        "    query_indices = []\n",
        "    all_query_data = []\n",
        "\n",
        "    # Function to find similar queries in Firestore\n",
        "    def get_similar_cached_response(new_query):\n",
        "        # Generate the embedding for the new query\n",
        "        new_query_embedding = model.encode([new_query])[0]\n",
        "\n",
        "        # Retrieve all cached queries and their embeddings from Firestore\n",
        "        query_cache_ref = db.collection('query_cache')\n",
        "        docs = query_cache_ref.stream()\n",
        "\n",
        "        # Initialize variables to track the most similar query and its response\n",
        "        most_similar_response = None\n",
        "        highest_similarity = 0\n",
        "        source = \"None\"\n",
        "\n",
        "        for doc in docs:\n",
        "            query_data = doc.to_dict()\n",
        "            cached_embedding = query_data.get('embedding')\n",
        "\n",
        "            if cached_embedding is None:\n",
        "                continue\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity([new_query_embedding], [cached_embedding])[0][0]\n",
        "\n",
        "            # If similarity is above threshold, consider it a match\n",
        "            if similarity > highest_similarity and similarity > 0.85:\n",
        "                highest_similarity = similarity\n",
        "                most_similar_response = query_data.get('response')\n",
        "                source = \"Firestore (Similar Match)\"\n",
        "\n",
        "        return most_similar_response, source, new_query_embedding.tolist()\n",
        "\n",
        "    # Process each query\n",
        "    for index, row in df.iterrows():\n",
        "        query = str(row[requirement_col]).strip()\n",
        "        query_indices.append(index)\n",
        "\n",
        "        if not query or query.lower() in [\"nan\", \"none\", \"null\"]:\n",
        "            all_query_data.append({\n",
        "                \"index\": index,\n",
        "                \"query\": query,\n",
        "                \"response\": \"No Valid Query Provided\",\n",
        "                \"source\": \"N/A\",\n",
        "                \"embedding\": None\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing query {index+1}/{len(df)}: {query[:50]}...\")\n",
        "\n",
        "        # First check for exact match in Firestore\n",
        "        doc_id = hashlib.sha256(query.encode('utf-8')).hexdigest()\n",
        "        doc_ref = db.collection('query_cache').document(doc_id)\n",
        "        doc = doc_ref.get()\n",
        "\n",
        "        if doc.exists:\n",
        "            # Add to cached queries list\n",
        "            response_text = doc.to_dict()['response']\n",
        "            source = \"Firestore (Exact Match)\"\n",
        "            all_query_data.append({\n",
        "                \"index\": index,\n",
        "                \"query\": query,\n",
        "                \"response\": response_text,\n",
        "                \"source\": source,\n",
        "                \"embedding\": doc.to_dict().get('embedding')\n",
        "            })\n",
        "            cached_queries.append(index)\n",
        "        else:\n",
        "            # Check for similar queries\n",
        "            similar_response, source, embedding = get_similar_cached_response(query)\n",
        "\n",
        "            if similar_response:\n",
        "                all_query_data.append({\n",
        "                    \"index\": index,\n",
        "                    \"query\": query,\n",
        "                    \"response\": similar_response,\n",
        "                    \"source\": source,\n",
        "                    \"embedding\": embedding\n",
        "                })\n",
        "                cached_queries.append(index)\n",
        "            else:\n",
        "                # Add to new queries list that need Discovery Engine processing\n",
        "                new_queries.append({\n",
        "                    \"index\": index,\n",
        "                    \"query\": query,\n",
        "                    \"embedding\": embedding\n",
        "                })\n",
        "\n",
        "    # Save the intermediate results\n",
        "    intermediate_file = f\"/tmp/query_processing_data.json\"\n",
        "    with open(intermediate_file, 'w') as f:\n",
        "        json.dump({\n",
        "            \"all_query_data\": all_query_data,\n",
        "            \"new_queries\": new_queries,\n",
        "            \"original_file\": processed_file_path\n",
        "        }, f)\n",
        "\n",
        "    # Upload to GCS\n",
        "    intermediate_gcs_path = f\"intermediate_data/query_filter_results_{processed_file_path.split('/')[-1].replace('.csv', '.json')}\"\n",
        "    blob = results_bucket_ref.blob(intermediate_gcs_path)\n",
        "    blob.upload_from_filename(intermediate_file)\n",
        "\n",
        "    print(f\"Found {len(cached_queries)} cached queries and {len(new_queries)} new queries that need processing\")\n",
        "\n",
        "    return [intermediate_gcs_path, processed_file_path]\n"
      ],
      "metadata": {
        "id": "fCLTWqpNvh45"
      },
      "id": "fCLTWqpNvh45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoke Vertex AI RAG for new or remaining queries"
      ],
      "metadata": {
        "id": "EuzSikkqwzgg"
      },
      "id": "EuzSikkqwzgg"
    },
    {
      "cell_type": "code",
      "source": [
        "#@dsl.component(base_image=\"python:3.9\")\n",
        "@dsl.component(base_image=\"gcr.io/deeplearning-platform-release/base-cpu\")\n",
        "def process_api_queries(\n",
        "    results_bucket: str,\n",
        "    input_data_paths: list,\n",
        "    firestore_project_id: str,\n",
        "    bigquery_table: str\n",
        ") -> str:\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "                   \"google-cloud-firestore\", \"google-cloud-bigquery\",\n",
        "                   \"google-cloud-storage\", \"pandas\", \"requests\", \"google-auth\"], check=True)\n",
        "\n",
        "    import pandas as pd\n",
        "    from google.cloud import firestore, bigquery, storage\n",
        "    import requests\n",
        "    import hashlib\n",
        "    import time\n",
        "    import json\n",
        "    from google.auth import default\n",
        "    from google.auth.transport.requests import Request\n",
        "\n",
        "\n",
        "    # Parse input paths\n",
        "    intermediate_gcs_path = input_data_paths[0]\n",
        "    processed_file_path = input_data_paths[1]\n",
        "\n",
        "    # Initialize Clients\n",
        "    storage_client = storage.Client()\n",
        "    db = firestore.Client(project=firestore_project_id)\n",
        "    bigquery_client = bigquery.Client(project=firestore_project_id)\n",
        "\n",
        "    # Function to get the access token for authentication\n",
        "    def get_access_token():\n",
        "\n",
        "      # Assuming this function is set up correctly to get the access token\n",
        "      credentials, _ = default()\n",
        "      credentials.refresh(Request())\n",
        "      return credentials.token\n",
        "\n",
        "\n",
        "    # Define Discovery Engine API endpoint\n",
        "    api_url = \"https://discoveryengine.googleapis.com/v1alpha/projects/994544669142/locations/global/collections/default_collection/engines/rfp-agent_1722488765661/servingConfigs/default_search:answer\"\n",
        "\n",
        "    # Define Paths\n",
        "    results_bucket_ref = storage_client.bucket(results_bucket)\n",
        "\n",
        "    # Download the intermediate data\n",
        "    local_intermediate_path = f\"/tmp/{intermediate_gcs_path.split('/')[-1]}\"\n",
        "    blob = results_bucket_ref.blob(intermediate_gcs_path)\n",
        "    blob.download_to_filename(local_intermediate_path)\n",
        "\n",
        "    with open(local_intermediate_path, 'r') as f:\n",
        "        intermediate_data = json.load(f)\n",
        "\n",
        "    all_query_data = intermediate_data[\"all_query_data\"]\n",
        "    new_queries = intermediate_data[\"new_queries\"]\n",
        "\n",
        "    # Download the original CSV to update at the end\n",
        "    local_csv_path = f\"/tmp/{processed_file_path.split('/')[-1]}\"\n",
        "    blob = results_bucket_ref.blob(processed_file_path)\n",
        "    blob.download_to_filename(local_csv_path)\n",
        "    df = pd.read_csv(local_csv_path)\n",
        "\n",
        "    # Function to call the Discovery Engine API\n",
        "    def call_discovery_engine(query_text, session_id=None):\n",
        "\n",
        "      access_token = get_access_token()\n",
        "      headers = {\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "               }\n",
        "\n",
        "      payload = {\n",
        "            \"query\": {\n",
        "                \"text\": query_text\n",
        "            },\n",
        "            \"session\": session_id or \"\",  # Pass empty session_id for the first query\n",
        "            \"answerGenerationSpec\": {\n",
        "                \"ignoreAdversarialQuery\": True,\n",
        "                \"ignoreNonAnswerSeekingQuery\": True,\n",
        "                \"ignoreLowRelevantContent\": True,\n",
        "                \"includeCitations\": True,\n",
        "                \"promptSpec\": {\n",
        "                    \"preamble\": (\n",
        "                        \"Please keep the answer concise and summary should be between 100 to max 150 words\"\n",
        "                        \"Treat the requirement of input as a question and provide summary\"\n",
        "                        \"Dont just provide Yes/No answer. Elaborate on the quesiton and answer thoroougly\"\n",
        "                        \"Dont use the names of the files or entities such as banks or a business entity in your answer.\"\n",
        "                        \"The resulting summary should answer the question or requirement without referring to the documents or sources as a professional sales person would answer the requirement\"\n",
        "\n",
        "                    )\n",
        "                },\n",
        "                \"modelSpec\": {\n",
        "                    \"modelVersion\": \"preview\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "      max_retries = 3\n",
        "      for attempt in range(max_retries):\n",
        "        try:\n",
        "          print(f\"Calling Discovery Engine API for: {query_text[:50]}...\")\n",
        "          response = requests.post(api_url, headers=headers, data=json.dumps(payload)) # Recent Update from notebook colab\n",
        "          response.raise_for_status()\n",
        "          return response.json()\n",
        "\n",
        "          # Check if response contains undesired message\n",
        "          if \"summary\" in response_json and \"A summary could not be generated for your search query\" in response_json[\"summary\"]:\n",
        "            print(f\"Skipping query due to invalid response: {query_text[:50]}...\")\n",
        "            return None  # Do not store this response\n",
        "\n",
        "          return response_json  # Valid response\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "          print(f\"API request attempt {attempt+1} failed: {str(e)}\")\n",
        "          if attempt < max_retries - 1:\n",
        "            time.sleep(2 * (attempt + 1))  # Exponential backoff\n",
        "            return None\n",
        "\n",
        "    # Process new queries in batches\n",
        "    batch_size = 10  # Adjust based on API quota limits\n",
        "    session_id = None  # For conversation continuity\n",
        "\n",
        "    print(f\"Processing {len(new_queries)} new queries with Discovery Engine API\")\n",
        "\n",
        "    for i in range(0, len(new_queries), batch_size):\n",
        "        batch = new_queries[i:i+batch_size]\n",
        "\n",
        "        for query_item in batch:\n",
        "            index = query_item[\"index\"]\n",
        "            query = query_item[\"query\"]\n",
        "            embedding = query_item[\"embedding\"]\n",
        "\n",
        "            # Call Discovery Engine\n",
        "            response_json = call_discovery_engine(query, session_id)\n",
        "\n",
        "            if response_json:\n",
        "                response_text = response_json.get(\"answer\", {}).get(\"answerText\", \"\")\n",
        "                # Update session ID for conversation continuity if needed\n",
        "                session_id = response_json.get(\"session\", session_id)\n",
        "                source = \"Discovery Engine\"\n",
        "\n",
        "                # Store in Firestore for future use\n",
        "                doc_id = hashlib.sha256(query.encode('utf-8')).hexdigest()\n",
        "                doc_ref = db.collection('query_cache').document(doc_id)\n",
        "                doc_ref.set({\n",
        "                    'query': query,\n",
        "                    'response': response_text,\n",
        "                    'timestamp': firestore.SERVER_TIMESTAMP,\n",
        "                    'embedding': embedding\n",
        "                })\n",
        "\n",
        "                # Store in BigQuery for analytics\n",
        "                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                try:\n",
        "                    bigquery_client.insert_rows_json(\n",
        "                        bigquery_table,\n",
        "                        [{\"query\": query, \"response\": response_text, \"timestamp\": timestamp, \"source\": source}]\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"BigQuery insert error: {str(e)}\")\n",
        "            else:\n",
        "                response_text = \"Error: Failed to get response from Discovery Engine API.\"\n",
        "                source = \"Error\"\n",
        "\n",
        "            # Add to all_query_data\n",
        "            all_query_data.append({\n",
        "                \"index\": index,\n",
        "                \"query\": query,\n",
        "                \"response\": response_text,\n",
        "                \"source\": source,\n",
        "                \"embedding\": embedding\n",
        "            })\n",
        "\n",
        "        # Add a small delay between batches to avoid rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Update DataFrame with all responses\n",
        "    df[\"AI_Response\"] = \"\"\n",
        "    df[\"Response_Source\"] = \"\"\n",
        "\n",
        "    for query_data in all_query_data:\n",
        "        idx = query_data[\"index\"]\n",
        "        df.at[idx, \"AI_Response\"] = query_data[\"response\"]\n",
        "        df.at[idx, \"Response_Source\"] = query_data[\"source\"]\n",
        "\n",
        "    # Save updated DataFrame\n",
        "    processed_output_path = f\"/tmp/processed_{processed_file_path.split('/')[-1]}\"\n",
        "    df.to_csv(processed_output_path, index=False)\n",
        "\n",
        "    # Upload to `processed_responses/` inside results bucket\n",
        "    output_gcs_path = f\"processed_responses/{processed_file_path.split('/')[-1]}\"\n",
        "    blob = results_bucket_ref.blob(output_gcs_path)\n",
        "    blob.upload_from_filename(processed_output_path)\n",
        "\n",
        "    print(f\"Processed {len(all_query_data)} queries total. Results saved to: {output_gcs_path}\")\n",
        "\n",
        "    return output_gcs_path\n"
      ],
      "metadata": {
        "id": "ePkXm3Zzvvl1"
      },
      "id": "ePkXm3Zzvvl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Component"
      ],
      "metadata": {
        "id": "R4asi-mOw7W_"
      },
      "id": "R4asi-mOw7W_"
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(base_image=\"python:3.9\")\n",
        "def log_pipeline_completion(pipeline_name: str, file_name: str):\n",
        "    print(f\"Pipeline {pipeline_name} completed processing file: {file_name}\")\n",
        "    print(f\"Timestamp: {__import__('time').strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KtdjuWyPv5zT"
      },
      "id": "KtdjuWyPv5zT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean up Cache if unwanted data stored in the Firestore"
      ],
      "metadata": {
        "id": "RRWkkhsZw9Yp"
      },
      "id": "RRWkkhsZw9Yp"
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(base_image=\"python:3.9\")\n",
        "def delete_invalid_firestore_responses(firestore_project_id: str, collection_name: str) -> int:\n",
        "    \"\"\"\n",
        "    A Kubeflow Pipeline component to delete invalid responses from Firestore.\n",
        "\n",
        "    Args:\n",
        "        firestore_project_id (str): Firestore Project ID.\n",
        "        collection_name (str): Name of the Firestore collection.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of deleted records.\n",
        "    \"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    # Install necessary dependencies inside the component\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\", \"google-cloud-firestore\"], check=True)\n",
        "    from google.cloud import firestore\n",
        "\n",
        "    # Initialize Firestore Client\n",
        "    db = firestore.Client(project=firestore_project_id)\n",
        "    collection_ref = db.collection(collection_name)\n",
        "\n",
        "    deleted_count = 0  # Track the number of deleted records\n",
        "\n",
        "    # Fetch all documents in the Firestore collection\n",
        "    docs = collection_ref.stream()\n",
        "\n",
        "    for doc in docs:\n",
        "        doc_data = doc.to_dict()  # Convert document to dictionary\n",
        "\n",
        "        # Check if 'response' exists and is a string before searching for the phrase\n",
        "        response_value = doc_data.get(\"response\", \"\")\n",
        "\n",
        "        if isinstance(response_value, str) and \"A summary could not be generated for your search query\" in response_value:\n",
        "            print(f\"Deleting document: {doc.id} due to invalid response\")\n",
        "            collection_ref.document(doc.id).delete()\n",
        "            deleted_count += 1\n",
        "\n",
        "    print(f\"Total deleted records: {deleted_count}\")\n",
        "    return deleted_count  # Return the number of deleted records\n"
      ],
      "metadata": {
        "id": "bNzmxr6QwJUQ"
      },
      "id": "bNzmxr6QwJUQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exit Handler"
      ],
      "metadata": {
        "id": "zT2ORuSQxEpE"
      },
      "id": "zT2ORuSQxEpE"
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(base_image=\"python:3.9\")\n",
        "def exit_handler_component(pipeline_name: str, file_name: str, status: str):\n",
        "    \"\"\"Logs pipeline completion status and can be used for cleanup tasks.\"\"\"\n",
        "    import time\n",
        "\n",
        "    print(f\"Pipeline {pipeline_name} finished with status: {status}\")\n",
        "    print(f\"Processed file: {file_name}\")\n",
        "    print(f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ],
      "metadata": {
        "id": "jCwJdzZpwMub"
      },
      "id": "jCwJdzZpwMub",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Pipeline"
      ],
      "metadata": {
        "id": "BJd3r8SkxHsW"
      },
      "id": "BJd3r8SkxHsW"
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.pipeline(name=\"optimized_rag_pipeline\")\n",
        "def optimized_rag_pipeline(\n",
        "    source_bucket: str,\n",
        "    results_bucket: str,\n",
        "    file_name: str,\n",
        "    firestore_project_id: str,\n",
        "    bigquery_table: str\n",
        "):\n",
        "    # ✅ Define an exit handler that logs success/failure\n",
        "    with dsl.ExitHandler(exit_handler_component(\n",
        "        pipeline_name=\"optimized_rag_pipeline\",\n",
        "        file_name=file_name,\n",
        "        status=\"FAILED\"  # Default to failed; updated below if successful\n",
        "    )):\n",
        "        # Step 1: Extract Queries & Save to `extracted_queries/`\n",
        "        extract_task = extract_queries_from_xlsx(\n",
        "            source_bucket=source_bucket,\n",
        "            file_name=file_name,\n",
        "            results_bucket=results_bucket\n",
        "        ).set_caching_options(enable_caching=False)\n",
        "\n",
        "        # Step 2a: Filter queries based on similarity to existing Firestore data\n",
        "        filter_task = filter_similar_queries(\n",
        "            results_bucket=results_bucket,\n",
        "            processed_file_path=extract_task.output,\n",
        "            firestore_project_id=firestore_project_id\n",
        "        ).set_caching_options(enable_caching=False)\n",
        "\n",
        "        # Step 2b: Process remaining queries with Discovery Engine API\n",
        "        process_task = process_api_queries(\n",
        "            results_bucket=results_bucket,\n",
        "            input_data_paths=filter_task.output,\n",
        "            firestore_project_id=firestore_project_id,\n",
        "            bigquery_table=bigquery_table\n",
        "        ).set_caching_options(enable_caching=False)\n",
        "\n",
        "        # Step 3 :\n",
        "        delete_invalid_firestore_responses(\n",
        "        firestore_project_id=\"veefin-ai-426106\",\n",
        "        collection_name=\"query_cache\").after(process_task)\n",
        "        \"\"\"\n",
        "        # Step 3: Save Final Output\n",
        "        save_results_to_gcs(\n",
        "            results_bucket=results_bucket,\n",
        "            processed_file_path=process_task.output\n",
        "        ).set_caching_options(enable_caching=False)\n",
        "        \"\"\"\n",
        "\n",
        "    # ✅ If pipeline completes successfully, change status to SUCCESS\n",
        "    exit_handler_component(\n",
        "        pipeline_name=\"optimized_rag_pipeline\",\n",
        "        file_name=file_name,\n",
        "        status=\"SUCCESS\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Tq4m6me7wNO_"
      },
      "id": "Tq4m6me7wNO_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Compilation"
      ],
      "metadata": {
        "id": "bFdQcpOXxLQK"
      },
      "id": "bFdQcpOXxLQK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the new pipeline\n",
        "from kfp.compiler import Compiler\n",
        "pipeline_file = \"optimized_rag_pipeline.json\"\n",
        "Compiler().compile(optimized_rag_pipeline, pipeline_file)\n",
        "print(f\"Pipeline compiled successfully: {pipeline_file}\")\n"
      ],
      "metadata": {
        "id": "x3zrYo8wwRg6"
      },
      "id": "x3zrYo8wwRg6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Test"
      ],
      "metadata": {
        "id": "NL4szknhxO_r"
      },
      "id": "NL4szknhxO_r"
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to run the pipeline\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.PipelineJob(\n",
        "    display_name=\"optimized-rag-pipeline\",\n",
        "    template_path=\"optimized_rag_pipeline.json\",\n",
        "    parameter_values={\n",
        "        \"source_bucket\": \"rfp_data_cloud_function\",\n",
        "        \"results_bucket\": \"rfp_pipeline_results\",\n",
        "        \"file_name\": \"pipeline_test_LOS.xlsx\",\n",
        "        \"firestore_project_id\": \"veefin-ai-426106\",\n",
        "        \"bigquery_table\": \"veefin-ai-426106.rfp_data.rfp_queries_responses_timestamps\"\n",
        "    },\n",
        "    enable_caching=False  # Disable caching for the entire pipeline\n",
        ").run()"
      ],
      "metadata": {
        "id": "VRzKafpgwVgv"
      },
      "id": "VRzKafpgwVgv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "tejan (17 Mar 2025, 11:40:08)",
      "collapsed_sections": [
        "RzMtRoS2vPbo",
        "uxFEUUGowbDi",
        "kZPqgoqRwfaq",
        "9OC3Vm9RwpzT",
        "EuzSikkqwzgg",
        "R4asi-mOw7W_",
        "RRWkkhsZw9Yp",
        "zT2ORuSQxEpE",
        "BJd3r8SkxHsW",
        "bFdQcpOXxLQK",
        "NL4szknhxO_r"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}